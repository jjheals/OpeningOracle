from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from scraper.Paths import Paths
from scraper.Scraper import Scraper
from math import ceil, floor
from os import listdir 
import json 

class OpeningGPT: 
    
    # STATIC Hyperparams 
    
    ''' INPUT_CUTOFF  := minimum number of tokens to process for a given iteration. smaller values will 
                         preserve context better at the risk of creating infinite loops, while larger
                         values will be safer but may lose more context
    '''
    INPUT_CUTOFF:int = 10
    
    ''' CHUNK_OFFSET := the offset from the end of the chunk to prevent indexing errors. can be set lower 
                        (i.e greater abs value, closer to neg infinity) but cannot be set higher than -10
                        or will cause indexing errors in the GPT model 
    '''
    CHUNK_OFFSET:int = -10 

    ''' MODEL_NAME := the name of the model to use from https://huggingface.co/models 
    '''
    MODEL_NAME:str = "facebook/bart-large-cnn"
    
    ''' CHUNK_SIZE := the maximum number of tokens the model can take at a time. defined by the model, 
                      not arbitrarily (https://huggingface.co/models)
    '''
    CHUNK_SIZE:int = 1024

    ''' MAX_SUM_SIZE, MIN_SUM_SIZE := the max/min length of a summary generated by the model (when given 
                                      a chunk of approx CHUNK_SIZE words)
    '''
    MAX_SUM_SIZE:int = 50
    MIN_SUM_SIZE:int = 10

    ''' LENGTH_PEN := to balance between generating longer or shorter sequences; used to prevent the 
                      model from favoring excessively short or long outputs, and is incorporated into 
                      the generation process as a factor that scales the log probabilities of the 
                      generated tokens based on the length of the sequence/input.
    '''
    LENGTH_PEN:float = 2.0 

    ''' NUM_BEAMS := determines the number of beams (partial hypotheses) to consider during the 
                     generation process; larger values increase the diversity of hypotheses explored
                     during decoding, potentially leading to better results at the  cost of increased 
                     computational complexity.
    '''
    NUM_BEAMS:int = 4

    # DYNAMIC Attributes
    model_name:str        # Model name from https://huggingface.co/models
    tokenizer:object      # Tokenizer object 
    model:object          # Model 
    summarizer:object     # Summarizer to provide the summarization of text from the model
    
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(OpeningGPT.MODEL_NAME)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(OpeningGPT.MODEL_NAME)
        self.summarizer = pipeline("summarization", model=self.model, tokenizer=self.tokenizer)
        
    
    ''' generate_summary(input_str) - generate a summary of the given string 
    
        --- DESCRIPTION --- 
            Take as input an input_str (str) and generate a summary for that string. If the length
            of the output (as in the number of tokens returned by self.tokenizer) is L, then: 
            
                min_sum_len <= L <= max_sum_len 
        
        --- PARAMETERS --- 
            
            input_str (str) - string to summarize
            
            min_sum_len (int) - optional - minimum length of the generated summary (as word count)
            
            max_sum_len (int) - optional - maximum length of the generated summary (as word count)
            
            full_summary (bool) - optional - specify whether to generate the final summary; set to False 
                                             if calling this function to precompute the summaries for an 
                                             opening to store for later use, set to true if calling for 
                                             testing or other purposes; default == True
                                             
        --- RETURNS --- 
            (str) A summary of the input_str.
    ''' 
    def generate_summary(self, input_str:str, min_sum_len:int=50, max_sum_len:int=200, full_summary:bool=True, print_debug=False, temperature:float=0.0) -> str:
        
        input_str = OpeningGPT.clean_input_str(input_str)
        
        # Concatenate all the summaries together to form a single string 
        concat_summary:str = self.__process_text__(input_str, min_len=min_sum_len, max_len=max_sum_len, temperature=temperature, print_debug=print_debug)
        
        # While concat summary is too large, keep condensing the summaries and regenerating it
        c = 0
        while len(self.tokenize(concat_summary)) > OpeningGPT.CHUNK_SIZE: 
            print(f"OpeningGPT: generate_summary - Iteration {c} | len of tokenized concat_summary = {len(self.tokenize(concat_summary))}")
            concat_summary = self.__process_text__(concat_summary, temperature=temperature, print_debug=print_debug)
            c+=1
        
        if full_summary: 
            # Now that concat_summary is small enough to pass through the model, pass it through one more time 
            # to generate a coherent output 
            concat_summary = self.__process_text__(concat_summary, temperature=temperature, print_debug=print_debug)
        
        return concat_summary
            
        
    def __process_text__(self, input_str:str, max_len:int=300, min_len:int=150, temperature:float=0.0, print_debug:bool=False) -> str:
        
        
        num_input_chars:int = len(input_str)
        input_toks = self.tokenize(input_str)
        num_input_toks:int = len(input_toks)
        tok_chars = sum([len(t) for t in input_toks])

        if print_debug: 
            print("\n[+] OpeningGPT: Called __process_text__")
            print(f"\tlen(test_input) = {len(input_str)}")
            print(f"\tlen(input_toks) = {len(input_toks)}")
            print(f"\tnum tok_chars = {tok_chars}")
            print(f"\ttemperature = {temperature}")

        do_sample:bool = bool(temperature)  # Included as a hyperparam to summarizer.summarize, only needed if temp is set
        
        summaries:list[str] = []    # List of all sub summaries generated from the chunks of text
        toks_processed:int = 0      # Counter of the number of tokens from the input text that have been processed
        chars_processed:int = 0     # Counter of the number of characters from the input text that have been processed
        c = 0                       # Counter for debugging prints
        
        # Loop while we still have characters to process 
        while chars_processed < len(input_str):
            if print_debug: print(f"\nOpeningGPT: __process_text__ - Iteration #{c}\n\tchars_processed = {chars_processed}/{num_input_chars}\n\ttoks_processed = {toks_processed}/{num_input_toks}")
            
            # Calculate how many characters we can take from the text (starting at input_str[chars_processed]) to get at most
            # [CHUNK_SIZE] tokens
            num_toks:int = len(self.tokenize(input_str[chars_processed : chars_processed + OpeningGPT.CHUNK_SIZE]))
            n:int = sum([len(t) for t in input_toks[toks_processed : toks_processed + OpeningGPT.CHUNK_SIZE]])
            num_chars = n - ceil(n/num_toks)*2
            
            # If num_chars is 0, then we've reached the end of the input, so we can break the loop
            if num_chars == 0 or num_toks < OpeningGPT.INPUT_CUTOFF: break
            
            # Ensure the chunk ends at a complete token; take less characters if necessary, never more
            while chars_processed + num_chars < len(input_str) and input_str[chars_processed + num_chars] != ' ':
                num_chars -= 1
            
            # If num_chars is 0, then we've reached the end of the input, so we can break the loop
            if num_chars == 0 or num_toks < OpeningGPT.INPUT_CUTOFF: break
            
            # Get the chunk from the input string, from the range (chars_processed, chars_processed + num_chars] 
            chunk:str = input_str[chars_processed : chars_processed + num_chars]
            chunk_len:int = len(self.tokenize(chunk))
            # Generate the summary and append it to the list of summaries
            try: 
                summary = self.summarizer(
                            chunk[:OpeningGPT.CHUNK_OFFSET], 
                            max_length=max_len, 
                            min_length=min_len, 
                            length_penalty=OpeningGPT.LENGTH_PEN, 
                            num_beams=OpeningGPT.NUM_BEAMS, 
                            early_stopping=True,
                            temperature=temperature,
                            do_sample=do_sample
                        )[0]['summary_text']
                summaries.append(summary)
            except IndexError: 
                if print_debug: print(f"OpeningGPT: __process_text__: Received an index error. Skipping this chunk.")
                    
                # Increment the variables before continuing to the next iteration 
                chars_processed += num_chars
                toks_processed += chunk_len
                c += 1
                continue
            
            if print_debug:
                print(f"\tn = {n}")
                print(f"\tnum_chars = {num_chars}")
                print(f"\tlen(tokenized chunk) = {chunk_len} | {sum([len(t) for t in self.tokenize(chunk)])}")
                print(f"\tsummary: \"{summary}\"")
                print(f"\tsummary len = {len(self.tokenize(summary))}")
            
            # Increment the variables before continuing to the next iteration 
            chars_processed += num_chars
            toks_processed += chunk_len
            c += 1
            
        if print_debug: 
            print(f"__process_text__() DONE. Returning the concatenated summary: \n\"{' '.join(summaries)}\"")
            
        return " ".join(summaries)
    
    def precompute_all_summaries(self, save_to:str=Paths.SUMMARIES_JSON, print_debug:bool=False, min_sum_len:int=50, max_sum_len:int=200, overwrite_existing:bool=False, temperature:float=0.0) -> None: 
        summaries:dict[str,str] = {} 
        
        # Check if we're overwriting what already exists, and if we are NOT, then load what we have
        if not overwrite_existing: summaries = json.load(open(save_to, "r"))

        # Iterate through the Paths.RAW_DESCS directory and create a summary of the file "concat.txt" for each ECO
        all_ecos:list[str] = listdir(Paths.RAW_DESC_BASE)
        for e in all_ecos: 
            
            # Skip this eco if it exists in summaries already 
            if e in summaries: 
                if print_debug: print(f"OpeningGPT: precompute_all_summaries - Skipping ECO \"{e}\" because it's summary has already been computed.")
                continue 

            if print_debug: 
                print(f"OpeningGPT: precompute_all_summaries - Generating summary for {e}...")
            
            # Try to open the concat file for this ECO. if it cannot be found, it is likely because we could not scrape 
            # information on this particular opening, so just skip this eco
            try: 
                with open(Paths.RAW_DESC_BASE + e + "/concat.txt", "r") as concat_file: 
                    this_desc:str = concat_file.read()
            except FileNotFoundError: 
                if print_debug: print(f"OpeningGPT: precompute_all_summaries - Skipping ECO \"{e}\" because the file \"concat.txt\" does not exist.")
                continue 
                
            # Generate a summary of the content in concat_file
            summary:str = self.generate_summary(this_desc, min_sum_len=min_sum_len, max_sum_len=max_sum_len, full_summary=False, print_debug=print_debug, temperature=temperature)
            
            # Add this summary to the summaries dict for this ECO
            summaries[e] = summary 

            if print_debug: print(f"Summary for {e}: \n\"{summary}\"")
            
            # Dump the final summaries dict to the file at the end of each iteration so that we can 
            # stop it in the middle if needed
            with open(save_to, "w") as summaries_json: 
                json.dump(summaries, summaries_json, indent=4) 
            
        if print_debug: 
            print(f"OpeningGPT: precompute_all_summaries DONE. Dumping resuts to \"{Paths.SUMMARIES_JSON}\".")
            
        # Dump the final summaries dict to the file 
        with open(Paths.SUMMARIES_JSON, "w") as summaries_json: 
            json.dump(summaries, summaries_json, indent=4) 
        
    def randomize_all_summaries(self, print_debug:bool=False, overwrite_existing:bool=False) -> None: 
        pre_summaries:dict[str,str] = json.load(open(Paths.SUMMARIES_JSON, "r"))    # Original summaries
        rand_summaries:dict[str,str] = {}                                           # Randomized summaries
        
        if not overwrite_existing: rand_summaries = json.load(open(Paths.RAND_SUMMARIES_JSON, "r"))
        
        for eco,summ in pre_summaries.items(): 
            if eco in rand_summaries: 
                if print_debug: print(f"OpeningGPT.randomize_all_summaries - skipping \"{eco}\" because its random summary has already been computed.")
                continue
            
            if not summ: 
                if print_debug: print(f"OpeningGPT.randomize_all_summaries - skipping \"{eco}\" because there is no pre-summary available.")
                continue
            
            if print_debug: print(f"OpeningGPT.randomize_all_sumamries - randomizing \"{eco}.\"")
            
            rand_summaries[eco] = self.generate_summary(summ, temperature=0.8, print_debug=print_debug, full_summary=False)
            json.dump(rand_summaries, open(Paths.RAND_SUMMARIES_JSON, "w"), indent=4)
        
        if print_debug: print("OpeningGPT.randomize_all_summaries - DONE.")
    
    ''' tokenize(input_str) - wrapper to use self.tokenizer to tokenize the string via the same method as the model ''' 
    def tokenize(self, input_str) -> list[str]: 
        return self.tokenizer.tokenize(input_str)
    
    ''' clean_input_str(input_str) - clean an input string to process by the model
    
        --- DESCRIPTION --- 
            Take as input an input_str (str), and return the input_str as a clean string to be 
            processed by the model.
            
            Steps: 
                1. Remove newlines 
                
        --- PARAMETERS --- 
            input_str (str) - string of raw input, i.e. a block of text read from a file
            
        --- RETURNS --- 
            (str) Clean string that can then be tokenized and passed to the model
    ''' 
    @staticmethod
    def clean_input_str(input_str:str) -> str: 
        return input_str.replace("\n", "")
    